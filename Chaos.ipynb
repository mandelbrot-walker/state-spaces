{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff94dcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imghdr import tests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "from dysts.flows import Lorenz\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "from src.models.sequence.ss.standalone.s4 import S4\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a35f510",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\n",
    "# Optimizer\n",
    "parser.add_argument('--lr', default=0.01, type=float, help='Learning rate')\n",
    "parser.add_argument('--weight_decay', default=0.01, type=float, help='Weight decay')\n",
    "# Scheduler\n",
    "parser.add_argument('--patience', default=10, type=float, help='Patience for learning rate scheduler')\n",
    "# Dataset\n",
    "parser.add_argument('--dataset', default='lorenz', choices=['lorenz', 'mnist', 'cifar10'], type=str, help='Dataset')\n",
    "parser.add_argument('--grayscale', action='store_true', help='Use grayscale CIFAR10')\n",
    "# Dataloader\n",
    "parser.add_argument('--num_workers', default=4, type=int, help='Number of workers to use for dataloader')\n",
    "parser.add_argument('--batch_size', default=64, type=int, help='Batch size')\n",
    "# Model\n",
    "parser.add_argument('--n_layers', default=1, type=int, help='Number of layers')\n",
    "parser.add_argument('--d_model', default=4, type=int, help='Model dimension')\n",
    "parser.add_argument('--dropout', default=0.2, type=float, help='Dropout')\n",
    "parser.add_argument('--prenorm', action='store_true', help='Prenorm')\n",
    "# General\n",
    "parser.add_argument('--resume', '-r', action='store_true', help='Resume from checkpoint')\n",
    "\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a16491",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_loss = 1e9  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "# Data\n",
    "print(f'==> Preparing {args.dataset} data..')\n",
    "\n",
    "def split_train_val(train, val_split):\n",
    "    train_len = int(len(train) * (1.0-val_split))\n",
    "    train, val = torch.utils.data.random_split(\n",
    "        train,\n",
    "        (train_len, len(train) - train_len),\n",
    "        generator=torch.Generator().manual_seed(42),\n",
    "    )\n",
    "    return train, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cafed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LorenzDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, rho_min=1, rho_max=100, number_of_samples=1024, beta=8/3, sigma=10):\n",
    "        self.lorenz = Lorenz()\n",
    "        self.lorenz.beta = beta\n",
    "        self.lorenz.sigma = sigma\n",
    "        self.rho_max = rho_max\n",
    "        self.rhos = np.random.uniform(low=rho_min, high=rho_max, size=number_of_samples).astype(np.float32)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.lorenz.rho = self.rhos[idx]\n",
    "        trajectory = torch.tensor(self.lorenz.make_trajectory(1000, resample=True, standardize=True), dtype=torch.float32) / 100.0\n",
    "        return trajectory, torch.tensor(self.lorenz.rho / self.rho_max, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rhos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55443fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset == 'lorenz':\n",
    "    trainset = LorenzDataset(rho_min=1, rho_max=100, number_of_samples=4096 * 4)\n",
    "    valset = LorenzDataset(rho_min=1, rho_max=100, number_of_samples=128)\n",
    "    testset = LorenzDataset(rho_min=1, rho_max=100, number_of_samples=128)\n",
    "    d_input = 3\n",
    "    d_output = 1\n",
    "\n",
    "elif args.dataset == 'cifar10':\n",
    " \n",
    "    if args.grayscale:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=122.6 / 255.0, std=61.0 / 255.0),\n",
    "            transforms.Lambda(lambda x: x.view(1, 1024).t())\n",
    "        ])\n",
    "    else:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "            transforms.Lambda(lambda x: x.view(3, 1024).t())\n",
    "        ])\n",
    "\n",
    "    # S4 is trained on sequences with no data augmentation!\n",
    "    transform_train = transform_test = transform\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=True, download=True, transform=transform_train)\n",
    "    trainset, _ = split_train_val(trainset, val_split=0.1)\n",
    "        \n",
    "    valset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=True, download=True, transform=transform_test)\n",
    "    _, valset = split_train_val(valset, val_split=0.1)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "    d_input = 3 if not args.grayscale else 1\n",
    "    d_output = 10\n",
    "\n",
    "elif args.dataset == 'mnist':\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.view(1, 784).t())\n",
    "    ])\n",
    "    transform_train = transform_test = transform\n",
    "\n",
    "    trainset = torchvision.datasets.MNIST(\n",
    "        root='./data', train=True, download=True, transform=transform_train)\n",
    "    trainset, _ = split_train_val(trainset, val_split=0.1)\n",
    "\n",
    "    valset = torchvision.datasets.MNIST(\n",
    "        root='./data', train=True, download=True, transform=transform_test)\n",
    "    _, valset = split_train_val(valset, val_split=0.1)\n",
    "\n",
    "    testset = torchvision.datasets.MNIST(\n",
    "        root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "    d_input = 1\n",
    "    d_output = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc615379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=128, shuffle=True, num_workers=args.num_workers)\n",
    "valloader = torch.utils.data.DataLoader(\n",
    "    valset, batch_size=128, shuffle=False, num_workers=args.num_workers)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=128, shuffle=False, num_workers=args.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaeb3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class S4Model(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_input, \n",
    "        d_output=10, \n",
    "        d_model=256, \n",
    "        n_layers=4, \n",
    "        dropout=0.2,\n",
    "        prenorm=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.prenorm = prenorm\n",
    "\n",
    "        # Linear encoder (d_input = 1 for grayscale and 3 for RGB)\n",
    "        self.encoder = nn.Linear(d_input, d_model)\n",
    "\n",
    "        # Stack S4 layers as residual blocks\n",
    "        self.s4_layers = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        self.dropouts = nn.ModuleList()\n",
    "        for _ in range(n_layers):\n",
    "            self.s4_layers.append(\n",
    "                S4(H=d_model, l_max=1024, dropout=dropout, transposed=True)\n",
    "            )\n",
    "            self.norms.append(nn.LayerNorm(d_model))\n",
    "            self.dropouts.append(nn.Dropout2d(dropout))\n",
    "\n",
    "        # Linear decoder\n",
    "        self.decoder = nn.Linear(d_model, d_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input x is shape (B, L, d_input)\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)  # (B, L, d_input) -> (B, L, d_model)\n",
    "        \n",
    "        x = x.transpose(-1, -2)  # (B, L, d_model) -> (B, d_model, L)\n",
    "        for layer, norm, dropout in zip(self.s4_layers, self.norms, self.dropouts):\n",
    "            # Each iteration of this loop will map (B, d_model, L) -> (B, d_model, L)\n",
    "\n",
    "            z = x\n",
    "            if self.prenorm:\n",
    "                # Prenorm\n",
    "                z = norm(z.transpose(-1, -2)).transpose(-1, -2)\n",
    "            \n",
    "            # Apply S4 block: we ignore the state input and output\n",
    "            z, _ = layer(z)\n",
    "\n",
    "            # Dropout on the output of the S4 block\n",
    "            z = dropout(z)\n",
    "\n",
    "            # Residual connection\n",
    "            x = z + x\n",
    "\n",
    "            if not self.prenorm:\n",
    "                # Postnorm\n",
    "                x = norm(x.transpose(-1, -2)).transpose(-1, -2)\n",
    "\n",
    "        x = x.transpose(-1, -2)\n",
    "\n",
    "        # Pooling: average pooling over the sequence length\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        # Decode the outputs\n",
    "        x = self.decoder(x)  # (B, d_model) -> (B, d_output)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a9f11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "print('==> Building model..')\n",
    "model = S4Model(\n",
    "    d_input=d_input, \n",
    "    d_output=d_output, \n",
    "    d_model=args.d_model, \n",
    "    n_layers=args.n_layers, \n",
    "    dropout=args.dropout,\n",
    "    prenorm=args.prenorm,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "if device == 'cuda':\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "if args.resume:\n",
    "    # Load checkpoint.\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
    "    checkpoint = torch.load('./checkpoint/ckpt.pth')\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    best_acc = checkpoint['acc']\n",
    "    start_epoch = checkpoint['epoch']\n",
    "\n",
    "def setup_optimizer(model, lr, weight_decay, patience):\n",
    "    \"\"\"\n",
    "    S4 requires a specific optimizer setup.\n",
    "\n",
    "    The S4 layer (A, B, C, dt) parameters typically \n",
    "    require a smaller learning rate (typically 0.001), with no weight decay. \n",
    "\n",
    "    The rest of the model can be trained with a higher learning rate (e.g. 0.004, 0.01) \n",
    "    and weight decay (if desired).\n",
    "    \"\"\"\n",
    "\n",
    "    # All parameters in the model\n",
    "    all_parameters = list(model.parameters())\n",
    "    \n",
    "    # General parameters don't contain the special _optim key\n",
    "    params = [p for p in all_parameters if not hasattr(p, \"_optim\")]\n",
    "\n",
    "    # Create an optimizer with the general parameters\n",
    "    optimizer = optim.AdamW(\n",
    "        params, \n",
    "        lr=lr, \n",
    "        weight_decay=weight_decay,\n",
    "    )\n",
    "\n",
    "    # Add parameters with special hyperparameters\n",
    "    hps = [getattr(p, \"_optim\") for p in all_parameters if hasattr(p, \"_optim\")]\n",
    "    hps = [\n",
    "        dict(s) for s in set(frozenset(hp.items()) for hp in hps)\n",
    "    ]  # Unique dicts\n",
    "    for hp in hps:\n",
    "        params = [p for p in all_parameters if getattr(p, \"_optim\", None) == hp]\n",
    "        optimizer.add_param_group(\n",
    "            {\"params\": params, **hp}\n",
    "        )\n",
    "\n",
    "    # Create a lr scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=patience, factor=0.2)\n",
    "    \n",
    "    # Print optimizer info \n",
    "    keys = sorted(set([k for hp in hps for k in hp.keys()]))\n",
    "    for i, g in enumerate(optimizer.param_groups):\n",
    "        group_hps = {k: g.get(k, None) for k in keys}\n",
    "        print(' | '.join([\n",
    "            f\"Optimizer group {i}\",\n",
    "            f\"{len(g['params'])} tensors\",\n",
    "        ] + [f\"{k} {v}\" for k, v in group_hps.items()]))\n",
    "\n",
    "    return optimizer, scheduler\n",
    "\n",
    "if args.dataset == 'lorenz':\n",
    "    criterion = nn.MSELoss()\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "optimizer, scheduler = setup_optimizer(\n",
    "    model, lr=args.lr, weight_decay=args.weight_decay, patience=args.patience\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83b2723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train():\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    pbar = tqdm(enumerate(trainloader))\n",
    "    for batch_idx, (inputs, targets) in pbar:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        pbar.set_description(\n",
    "            'Batch Idx: (%d/%d) | Loss: %.3f' % \n",
    "            (batch_idx, len(trainloader), train_loss/(batch_idx+1))\n",
    "        )\n",
    "\n",
    "\n",
    "def eval(epoch, dataloader, checkpoint=False):\n",
    "    global best_loss\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(enumerate(dataloader))\n",
    "        for batch_idx, (inputs, targets) in pbar:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            pbar.set_description(\n",
    "                'Batch Idx: (%d/%d) | Loss: %.3f' % \n",
    "                (batch_idx, len(dataloader), eval_loss/(batch_idx+1))\n",
    "            )\n",
    "\n",
    "    # Save checkpoint.\n",
    "    if checkpoint:\n",
    "        if eval_loss < best_loss:\n",
    "            state = {\n",
    "                'model': model.state_dict(),\n",
    "                'loss': eval_loss,\n",
    "                'epoch': epoch,\n",
    "            }\n",
    "            if not os.path.isdir('checkpoint'):\n",
    "                os.mkdir('checkpoint')\n",
    "            torch.save(state, './checkpoint/ckpt.pth')\n",
    "            best_loss = eval_loss\n",
    "\n",
    "        return eval_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545e40db",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm(range(start_epoch, start_epoch+200))\n",
    "for epoch in pbar:\n",
    "    if epoch == 0:\n",
    "        pbar.set_description('Epoch: %d' % (epoch))\n",
    "    else:\n",
    "        pbar.set_description('Epoch: %d | Val loss: %1.3f' % (epoch, val_loss))\n",
    "    train()\n",
    "    val_loss = eval(epoch, valloader, checkpoint=True)\n",
    "    eval(epoch, testloader)\n",
    "    scheduler.step(val_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
